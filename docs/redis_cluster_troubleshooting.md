# Troubleshooting Redis Cluster Formation Analysis

## Introduction

The allure of a Redis cluster lies in its robustness and scalability, offering increased power and redundancy. However, with these advantages comes an inevitable rise in complexity. This complexity, a natural byproduct of distributed systems, often becomes a breeding ground for potential errors and operational challenges. Troubleshooting such a system, therefore, requires a nuanced approach, blending methodical analysis with an understanding of Redis' intricacies.

This guide delves into the troubleshooting process used to identify and resolve issues following significant changes to the infrastructure code of a Redis cluster. While the specific challenges discussed may vary from one setup to another, the underlying principles, tools, and log analysis techniques are broadly applicable. They serve as a beacon, guiding through the often murky waters of cluster formation and maintenance.

Whether you're setting up a new cluster, scaling an existing one, or navigating through post-deployment hiccups, this guide aims to equip you with the knowledge to efficiently diagnose and resolve issues. It's tailored to help both newcomers easing into Redis cluster management and seasoned professionals seeking to refine their troubleshooting strategies. 

## Methodology

When troubleshooting a Redis cluster, several key resources and tools are essential for a deep dive into the system's state and behavior. Here's a guide to effectively utilize these resources:

### Redis CLI - Your Gateway to Cluster Insights
The Redis Command Line Interface (CLI) is an indispensable tool for interacting with and managing Redis clusters. In a bosh managed deployment it's typically located at /var/vcap/packages/redis-6/bin/redis-cli. Key options include:

* --tls: Tests the TLS (Transport Layer Security) configuration.
* --cacert: Specifies the location of the Certificate Authority (CA) file.
* --cert: Specifies the location of the SSL certificate.
* --key: Specifies the location of the private key file.
* --h: Host address of the Redis node.
* --p: Port number for the connection.
* --a: Password for authentication.

### Configuration Files - The Blueprint of Your Setup
Redis configuration settings are crucial for understanding the node or cluster setup. These configurations can be found at /var/vcap/jobs/cluster-6/config/redis.conf. This file contains settings related to network, memory management, security, and other operational parameters.

### Log Files - The Chronicle of Redis Operations
Log files are invaluable for diagnosing issues and understanding the operational history of the cluster. In BOSH deployments, logs are located at /var/vcap/sys/log/cluster-6/. Focus on:

**cluster-6.log**: The standard Redis log file providing detailed insights into the operations and events within the Redis cluster.

**post-deploy.stderr.log**: Generated by the script at /var/vcap/jobs/cluster-6/bin/post-deploy, this log file contains information related to the cluster's configuration process.

## Scenario 1
***Cluster, single master, single replica configuration (2 nodes)***

### Context

In this scenario, we expected to set up a Redis cluster with two nodes: one master and one replica. The master node should manage all 16384 cluster slots, and each cluster member should be aware of both itself and its counterpart (master or replica).

### Observations

Upon reviewing the cluster status post-implementation, we observed the following:

```
node/515c581c-596c-4d40-907b-7f2e1e6d9997:~# /var/vcap/packages/redis-6/bin/redis-cli --tls --cacert /var/vcap/jobs/cluster-6/config/tls/redis.ca --cert /var/vcap/jobs/cluster-6/config/tls/redis.cert --key /var/vcap/jobs/cluster-6/config/tls/redis.key -h 127.0.0.1 -p 6379 -a Zd3zcsp4kHh0aNHBGwUttcagZKDGb3QaCqhLvAxDAMlDcImWDNXUlCOLti6ELGi3
Warning: Using a password with '-a' or '-u' option on the command line interface may not be safe.
127.0.0.1:6379> cluster info
cluster_state:ok
cluster_slots_assigned:16384
cluster_slots_ok:16384
cluster_slots_pfail:0
cluster_slots_fail:0
cluster_known_nodes:1
cluster_size:1
cluster_current_epoch:0
cluster_my_epoch:0
cluster_stats_messages_sent:0
cluster_stats_messages_received:0
127.0.0.1:6379> cluster nodes
bfc3177710a27823762881570ce93e62ac6fd751 :6379@16379 myself,master - 0 0 0 connected 0-16383
127.0.0.1:6379>
```
* All slots are assigned to a single master, as expected in a single-master setup.
* It reports only 1 nodes in `cluster_known_nodes:1`, also in cluster nodes it only knows about `myself,master`.

At the same time we can see the standard Redis log reporting the startup:

```node/515c581c-596c-4d40-907b-7f2e1e6d9997:~# cat /var/vcap/sys/log/cluster-6/cluster-6.log 
9830:C 03 Jan 2024 15:26:23.021 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
9830:C 03 Jan 2024 15:26:23.021 # Redis version=6.2.12, bits=64, commit=00000000, modified=0, pid=9830, just started
9830:C 03 Jan 2024 15:26:23.021 # Configuration loaded
9830:M 03 Jan 2024 15:26:23.028 * Increased maximum number of open files to 10032 (it was originally set to 1024).
9830:M 03 Jan 2024 15:26:23.028 * monotonic clock: POSIX clock_gettime
9830:M 03 Jan 2024 15:26:23.033 # Warning: Could not create server TCP listening socket ::*:6379: unable to bind socket, errno: 97
9830:M 03 Jan 2024 15:26:23.034 * No cluster configuration found, I'm bfc3177710a27823762881570ce93e62ac6fd751
9830:M 03 Jan 2024 15:26:23.046 # Warning: Could not create server TCP listening socket ::*:16379: unable to bind socket, errno: 97
9830:M 03 Jan 2024 15:26:23.047 * Running mode=cluster, port=6379.
9830:M 03 Jan 2024 15:26:23.047 # Server initialized
9830:M 03 Jan 2024 15:26:23.047 * Ready to accept connections
9830:M 03 Jan 2024 15:26:51.553 # Cluster state changed: ok
```

* The log indicates a normal startup but lacks any entries about replication or connections to a replica node.

whereas `post-deploy.stderr.log` comes back noisy especially in regards to introducing nodes to each other:

```
node/515c581c-596c-4d40-907b-7f2e1e6d9997:~# cat /var/vcap/sys/log/cluster-6/post-deploy.stderr.log
[Wed Jan  3 15:26:46 UTC 2024] found cluster member 515c581c-596c-4d40-907b-7f2e1e6d9997.node.blacksmith.redis-clsuter-6-64134c80-bb13-4f11-977e-26fc50c68057.bosh:6379
[Wed Jan  3 15:26:46 UTC 2024] found cluster member 50fe4459-dc7b-4bc7-bdd5-3d05f3acc052.node.blacksmith.redis-clsuter-6-64134c80-bb13-4f11-977e-26fc50c68057.bosh:6379
[Wed Jan  3 15:26:46 UTC 2024] checking if clustering is enabled
[Wed Jan  3 15:26:46 UTC 2024] clustering is enabled on all nodes
[Wed Jan  3 15:26:46 UTC 2024] checking if clustering is already configured
OK
bfc3177710a27823762881570ce93e62ac6fd751 :6379@16379 myself,master - 0 0 0 connected
OK
199e46373e4ad7d6f823dec5461d8f4a44e86248 :6379@16379 myself,master - 0 0 0 connected
[Wed Jan  3 15:26:46 UTC 2024] clustering is not yet configured
[Wed Jan  3 15:26:46 UTC 2024] introducing nodes to each other
OK
ERR Invalid node address specified: 50fe4459-dc7b-4bc7-bdd5-3d05f3acc052.node.blacksmith.redis-clsuter-6-64134c80-bb13-4f11-977e-26fc50c68057.bosh:6379

[Wed Jan  3 15:26:46 UTC 2024] node introductions complete
OK
OK
OK
ERR Unknown node bfc3177710a27823762881570ce93e62ac6fd751

[Wed Jan  3 15:26:46 UTC 2024] configuration set
[Wed Jan  3 15:26:46 UTC 2024] waiting for all nodes to be connected to all other nodes
[Wed Jan  3 15:26:46 UTC 2024] cluster settled
OK
bfc3177710a27823762881570ce93e62ac6fd751 :6379@16379 myself,master - 0 0 0 connected 0-16383
OK
199e46373e4ad7d6f823dec5461d8f4a44e86248 :6379@16379 myself,master - 0 0 0 connected
node/515c581c-596c-4d40-907b-7f2e1e6d9997:~# 
```

* Errors during node introduction suggest issues in cluster formation, particularly with node addressing.

## Scenario 2
***Cluster, two masters, single replica configuration (4 nodes)***

### Context

In this four-node setup, we anticipate:

* Two master nodes, each managing half of the total cluster slots (8192 slots each).
* Each node should recognize itself and its corresponding master or replica.

### Observations

Again, our cluster info and node return a much different screen than the expected:

```
node/1954302a-3c2f-4b9f-a890-6a8c42798a5f:~# /var/vcap/packages/redis-6/bin/redis-cli --tls --cacert /var/vcap/jobs/cluster-6/config/tls/redis.ca --cert /var/vcap/jobs/cluster-6/config/tls/redis.cert --key /var/vcap/jobs/cluster-6/config/tls/redis.key -h 127.0.0.1 -p 6379 -a DOIUtihirx9aW2h4Dwr6T2JI4lBwVYE50b1SdJxlRblzgcORDKfMSNmP0HJs7BQw
Warning: Using a password with '-a' or '-u' option on the command line interface may not be safe.
127.0.0.1:6379> cluster info
cluster_state:fail
cluster_slots_assigned:8192
cluster_slots_ok:8192
cluster_slots_pfail:0
cluster_slots_fail:0
cluster_known_nodes:1
cluster_size:1
cluster_current_epoch:0
cluster_my_epoch:0
cluster_stats_messages_sent:0
cluster_stats_messages_received:0
127.0.0.1:6379> cluster nodes
868498305954d41936349b374f9647b7e7cd8c9b :6379@16379 myself,master - 0 0 0 connected 8192-16383
127.0.0.1:6379> 
```

* The cluster state is `fail``, indicating a non-operational cluster.
* Only half of the total cluster slots are assigned, and the cluster reports only one known node.
* Similar to Scenario 1, the cluster only acknowledges the presence of one master node.

The standard Redis log shows the startup and the node itself ready to accept connections:

```
node/1954302a-3c2f-4b9f-a890-6a8c42798a5f:~# cat /var/vcap/sys/log/cluster-6/cluster-6.log
9857:C 09 Jan 2024 08:49:53.833 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
9857:C 09 Jan 2024 08:49:53.833 # Redis version=6.2.12, bits=64, commit=00000000, modified=0, pid=9857, just started
9857:C 09 Jan 2024 08:49:53.833 # Configuration loaded
9857:M 09 Jan 2024 08:49:53.841 * Increased maximum number of open files to 10032 (it was originally set to 1024).
9857:M 09 Jan 2024 08:49:53.841 * monotonic clock: POSIX clock_gettime
9857:M 09 Jan 2024 08:49:53.846 # Warning: Could not create server TCP listening socket ::*:6379: unable to bind socket, errno: 97
9857:M 09 Jan 2024 08:49:53.847 * No cluster configuration found, I'm 868498305954d41936349b374f9647b7e7cd8c9b
9857:M 09 Jan 2024 08:49:53.857 # Warning: Could not create server TCP listening socket ::*:16379: unable to bind socket, errno: 97
9857:M 09 Jan 2024 08:49:53.858 * Running mode=cluster, port=6379.
9857:M 09 Jan 2024 08:49:53.858 # Server initialized
9857:M 09 Jan 2024 08:49:53.858 * Ready to accept connections
```

* The log indicates a standard startup without any mention of node connections or replication activities. 

Finally `post-deploy.stderr.log` confirms our findings so far:

```
node/1dd863bf-bc6b-47f5-932e-5c7b7f2cb2ca:~# cat /var/vcap/sys/log/cluster-6/post-deploy.stderr.log
[Tue Jan  9 08:50:17 UTC 2024] found cluster member 1dd863bf-bc6b-47f5-932e-5c7b7f2cb2ca.node.blacksmith.redis-cluster-big-aec824b9-b106-433f-a18a-1a2430866b11.bosh:6379
[Tue Jan  9 08:50:17 UTC 2024] found cluster member 1954302a-3c2f-4b9f-a890-6a8c42798a5f.node.blacksmith.redis-cluster-big-aec824b9-b106-433f-a18a-1a2430866b11.bosh:6379
[Tue Jan  9 08:50:17 UTC 2024] found cluster member 6e95172f-1da4-4b85-a459-ba270516f78f.node.blacksmith.redis-cluster-big-aec824b9-b106-433f-a18a-1a2430866b11.bosh:6379
[Tue Jan  9 08:50:17 UTC 2024] found cluster member f2e35fc2-1441-42b2-9850-3e0c58c1ca5f.node.blacksmith.redis-cluster-big-aec824b9-b106-433f-a18a-1a2430866b11.bosh:6379
[Tue Jan  9 08:50:17 UTC 2024] checking if clustering is enabled
[Tue Jan  9 08:50:17 UTC 2024] clustering is enabled on all nodes
[Tue Jan  9 08:50:17 UTC 2024] checking if clustering is already configured
OK
e66918bd2779dba2ba49edff147afa1237ec23a7 :6379@16379 myself,master - 0 0 0 connected
OK
868498305954d41936349b374f9647b7e7cd8c9b :6379@16379 myself,master - 0 0 0 connected
OK
5b0022f41dfce0782229175bcf567c882ef8d6ac :6379@16379 myself,master - 0 0 0 connected
OK
e1ac1fefff86d312fce986c7b4f95ade158f7308 :6379@16379 myself,master - 0 0 0 connected
[Tue Jan  9 08:50:17 UTC 2024] clustering is not yet configured
[Tue Jan  9 08:50:17 UTC 2024] introducing nodes to each other
OK
ERR Invalid node address specified: 1954302a-3c2f-4b9f-a890-6a8c42798a5f.node.blacksmith.redis-cluster-big-aec824b9-b106-433f-a18a-1a2430866b11.bosh:6379

OK
ERR Invalid node address specified: 6e95172f-1da4-4b85-a459-ba270516f78f.node.blacksmith.redis-cluster-big-aec824b9-b106-433f-a18a-1a2430866b11.bosh:6379

OK
ERR Invalid node address specified: f2e35fc2-1441-42b2-9850-3e0c58c1ca5f.node.blacksmith.redis-cluster-big-aec824b9-b106-433f-a18a-1a2430866b11.bosh:6379

OK
ERR Invalid node address specified: 6e95172f-1da4-4b85-a459-ba270516f78f.node.blacksmith.redis-cluster-big-aec824b9-b106-433f-a18a-1a2430866b11.bosh:6379

OK
ERR Invalid node address specified: f2e35fc2-1441-42b2-9850-3e0c58c1ca5f.node.blacksmith.redis-cluster-big-aec824b9-b106-433f-a18a-1a2430866b11.bosh:6379

OK
ERR Invalid node address specified: f2e35fc2-1441-42b2-9850-3e0c58c1ca5f.node.blacksmith.redis-cluster-big-aec824b9-b106-433f-a18a-1a2430866b11.bosh:6379

[Tue Jan  9 08:50:17 UTC 2024] node introductions complete
OK
OK
OK
OK
OK
ERR Unknown node e66918bd2779dba2ba49edff147afa1237ec23a7

OK
ERR Unknown node 868498305954d41936349b374f9647b7e7cd8c9b

[Tue Jan  9 08:50:18 UTC 2024] configuration set
[Tue Jan  9 08:50:18 UTC 2024] waiting for all nodes to be connected to all other nodes
[Tue Jan  9 08:50:18 UTC 2024] cluster settled
OK
e66918bd2779dba2ba49edff147afa1237ec23a7 :6379@16379 myself,master - 0 0 0 connected 0-8191
OK
868498305954d41936349b374f9647b7e7cd8c9b :6379@16379 myself,master - 0 0 0 connected 8192-16383
OK
5b0022f41dfce0782229175bcf567c882ef8d6ac :6379@16379 myself,master - 0 0 0 connected
OK
e1ac1fefff86d312fce986c7b4f95ade158f7308 :6379@16379 myself,master - 0 0 0 connected
node/1dd863bf-bc6b-47f5-932e-5c7b7f2cb2ca:~#
```

* Repeated ERR Invalid node address errors during node introductions hint at configuration or DNS resolution issues.
* Despite the errors, the script reports cluster settled, which conflicts with the actual cluster state.

## Issues

### DNS Usage in CLUSTER MEET Command
---
During our troubleshooting, we identified a significant misconfiguration: nodes were attempting to use DNS entries instead of IP addresses for the CLUSTER MEET command. This led to the error:

```
ERR Invalid node address specified: 50fe4459-dc7b-4bc7-bdd5-3d05f3acc052.node.blacksmith.redis-clsuter-6-64134c80-bb13-4f11-977e-26fc50c68057.bosh:6379
```

Redis requires IP addresses for node introductions, as specified in the [Redis documentation](https://redis.io/commands/cluster-meet/):

```re
CLUSTER MEET ip port [cluster-bus-port]
```

#### Resolution: DNS Resolution and Efficient Node Introduction

To address this, we modified the post-deploy script to resolve DNS names to IP addresses and ensure that each node pair is introduced only once. This approach eliminates the ERR Invalid node address issue and streamlines the cluster formation process.

*Key Script Updates*:

**Export DNS Entries**

* `bash_ips = ips.join(' ')`: This line exports the DNS entries outside the ERB template.

* `IPS_ARRAY=(<%= bash_ips %>)`: Makes the DNS entries available in the bash script.

**Node Introduction Logic**

1. The script now iterates over the DNS array, resolves each DNS name to its corresponding IP address, and performs the `CLUSTER MEET`` command with these IPs.
2. The introduction process is optimized to ensure each node pair is introduced only once, enhancing efficiency.

```
len=${#IPS_ARRAY[@]}
for ((i = 0; i < len; i++)); do
    for ((j = i + 1; j < len; j++)); do
        a=${IPS_ARRAY[i]}
        b=${IPS_ARRAY[j]}

        # Resolve DNS to IP
        ip_a=$(dig +short "$a")
        ip_b=$(dig +short "$b")

        if [ -n "$ip_a" ] && [ -n "$ip_b" ]; then
            log "Introducing $a ($ip_a) to $b ($ip_b)"
            redis "$ip_a" CLUSTER MEET "$ip_b" 6379
        else
            log "Failed to resolve DNS for $a or $b"
        fi
    done
done
```

### TLS Communication Error
---

During our investigation, we encountered the following error in the Redis logs:

```
node/6e4b3138-c2f7-4ecf-91d1-a896407655f3:~# head -n 200 /var/vcap/sys/log/cluster-6/cluster-6.log
9853:C 04 Jan 2024 11:29:56.299 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
9853:C 04 Jan 2024 11:29:56.299 # Redis version=6.2.12, bits=64, commit=00000000, modified=0, pid=9853, just started
9853:C 04 Jan 2024 11:29:56.299 # Configuration loaded
9853:M 04 Jan 2024 11:29:56.307 * Increased maximum number of open files to 10032 (it was originally set to 1024).
9853:M 04 Jan 2024 11:29:56.307 * monotonic clock: POSIX clock_gettime
9853:M 04 Jan 2024 11:29:56.311 # Warning: Could not create server TCP listening socket ::*:6379: unable to bind socket, errno: 97
9853:M 04 Jan 2024 11:29:56.312 * No cluster configuration found, I'm d133fcb29625580775948cdff631de559d94e602
9853:M 04 Jan 2024 11:29:56.322 # Warning: Could not create server TCP listening socket ::*:16379: unable to bind socket, errno: 97
9853:M 04 Jan 2024 11:29:56.323 * Running mode=cluster, port=6379.
9853:M 04 Jan 2024 11:29:56.323 # Server initialized
9853:M 04 Jan 2024 11:29:56.323 * Ready to accept connections
9853:M 04 Jan 2024 11:30:19.963 # IP address for this node updated to 10.168.17.8
9853:M 04 Jan 2024 11:30:20.256 # Error accepting a client connection: error:0A00010B:SSL routines::wrong version number
```

`Error accepting a client connection: error:0A00010B:SSL routines::wrong version number`: This error pointed to a problem with TLS communication, which was initially misleading due to the mention of a "wrong version number."


#### Resolution: Enabling TLS for Replication

As per [Redis's documentation](https://redis.io/docs/management/security/encryption/), _A Redis master server handles connecting clients and replica servers in the same way, so the above tls-port and tls-auth-clients directives apply to replication links as well.
On the replica server side, it is necessary to specify tls-replication yes to use TLS for outgoing connections to the master._

**Key Configuration Update**:

* `tls-replication yes` was added to `jobs/cluster-6/templates/config/redis.conf`. This change ensures that TLS is used for both incoming client connections and outgoing connections to the master server in a replication setup.


### SYNC Command Disabled
---

With TLS communication issues resolved, we encountered a new problem during the replication process:

```
9854:S 04 Jan 2024 12:34:07.790 * Connecting to MASTER 10.168.17.8:6379
9854:S 04 Jan 2024 12:34:07.790 * MASTER <-> REPLICA sync started
9854:S 04 Jan 2024 12:34:07.790 * Non blocking connect for SYNC fired the event.
9854:S 04 Jan 2024 12:34:07.791 # Error reply to PING from master: '-Reading from master: Connection reset by peer'
```

This error indicated a failure in the replication process, specifically during the synchronization step. Upon investigation, we discovered that the `SYNC`` command, essential for replication, had been disabled in our configuration.

#### Resolution: Re-enabling the SYNC Command

Redis replication relies on the `SYNC`` command to synchronize data between the master and its replicas. Disabling this command disrupts the replication process and prevents proper cluster operation.

To resolve this, we updated the Redis configuration by removing the `SYNC` command from the list of disabled commands in `jobs/cluster/spec` and `jobs/cluster-6/spec`

### Authentication Errors during Replication
---

After enabling the SYNC command, we encountered authentication errors during the replication process:

```
10608:S 04 Jan 2024 14:27:11.527 * MASTER <-> REPLICA sync started
10608:S 04 Jan 2024 14:27:11.530 * Non blocking connect for SYNC fired the event.
10608:S 04 Jan 2024 14:27:11.531 * Master replied to PING, replication can continue...
10608:S 04 Jan 2024 14:27:11.531 * (Non critical) Master does not understand REPLCONF listening-port: -NOAUTH Authentication required.
10608:S 04 Jan 2024 14:27:11.531 * (Non critical) Master does not understand REPLCONF capa: -NOAUTH Authentication required.
10608:S 04 Jan 2024 14:27:11.531 * Partial resynchronization not possible (no cached master)
10608:S 04 Jan 2024 14:27:11.532 # Unexpected reply to PSYNC from master: -NOAUTH Authentication required.
10608:S 04 Jan 2024 14:27:11.532 * Retrying with SYNC...
10608:S 04 Jan 2024 14:27:11.532 # MASTER aborted replication with an error: NOAUTH Authentication required.
```

This indicated that the replica was unable to authenticate with the master, halting the replication process.

#### Resolution: Configuring masterauth

As per the [documentation](https://redis.io/docs/management/replication/), Redis requires authentication for replication if the master is protected by a password (set via `requirepass`). To facilitate authentication during replication, the `masterauth` directive needs to be configured on the replica nodes.

We updated the Redis configuration files `jobs/cluster-6/templates/config/redis.conf` and `jobs/cluster/templates/config/redis.conf` to include the master's password:

`masterauth <%= p('auth.password') %>`

This configuration ensures that the replica nodes use the correct password to authenticate with the master during replication.

###  Incorrect Slave Assignment
---

In the initial setup of a Redis cluster with two masters and one replica for each master, we observed an anomaly. Only one instance was being designated as a slave, as evidenced by the cluster nodes output:

```
OK
7d590d90c6f3be3394f6b438cb301677504d3da7 10.168.17.13:6379@16379 master - 0 1704721333038 0 connected
b87c6c7cd2f5ad90e2a195367cc0d19b2db9058b 10.168.17.15:6379@16379 master - 0 1704721334047 0 connected
4f4f0ee5311fb2e95d043797933cec059d7d55a2 10.168.17.14:6379@16379 master - 0 1704721333000 0 connected
730299ba3127e0e89176e7cbd7498bf9e424b889 10.168.17.12:6379@16379 myself,master - 0 1704721333000 1 connected 0-8191
OK
7d590d90c6f3be3394f6b438cb301677504d3da7 10.168.17.13:6379@16379 myself,master - 0 1704721333000 2 connected 8192-16383
b87c6c7cd2f5ad90e2a195367cc0d19b2db9058b 10.168.17.15:6379@16379 master - 0 1704721333145 0 connected
4f4f0ee5311fb2e95d043797933cec059d7d55a2 10.168.17.14:6379@16379 slave 730299ba3127e0e89176e7cbd7498bf9e424b889 0 1704721333334 1 connected
730299ba3127e0e89176e7cbd7498bf9e424b889 10.168.17.12:6379@16379 master - 0 1704721332653 1 connected
OK
b87c6c7cd2f5ad90e2a195367cc0d19b2db9058b 10.168.17.15:6379@16379 master - 0 1704721333265 0 connected
730299ba3127e0e89176e7cbd7498bf9e424b889 10.168.17.12:6379@16379 master - 0 1704721332759 0 connected
4f4f0ee5311fb2e95d043797933cec059d7d55a2 10.168.17.14:6379@16379 myself,slave 7d590d90c6f3be3394f6b438cb301677504d3da7 0 1704721333000 2 connected
7d590d90c6f3be3394f6b438cb301677504d3da7 10.168.17.13:6379@16379 master - 0 1704721333655 2 connected 8192-16383
OK
7d590d90c6f3be3394f6b438cb301677504d3da7 10.168.17.13:6379@16379 master - 0 1704721333189 0 connected
4f4f0ee5311fb2e95d043797933cec059d7d55a2 10.168.17.14:6379@16379 master - 0 1704721333294 0 connected
730299ba3127e0e89176e7cbd7498bf9e424b889 10.168.17.12:6379@16379 master - 0 1704721333779 1 connected 0-8191
b87c6c7cd2f5ad90e2a195367cc0d19b2db9058b 10.168.17.15:6379@16379 myself,master - 0 1704721332000 0 connected
```

The corresponding generated `/var/vcap/jobs/cluster-6/bin/post-deploy` script indicated only one slave being assigned:

```sh
# set up slaves
MASTER=$(redis "9d2c3478-30b2-4293-9398-9255dc72803a.node.blacksmith.redis-cluster-big-d40ed344-e4dd-4ae8-b9bb-87d461405c21.bosh" CLUSTER NODES | grep myself | awk '{print $1}')
redis 6573d26c-225f-4a12-ad63-dfdc09f1ceaa.node.blacksmith.redis-cluster-big-d40ed344-e4dd-4ae8-b9bb-87d461405c21.bosh CLUSTER REPLICATE "$MASTER"
MASTER=$(redis "5586644d-6e23-49f2-bc9a-1cdd7b746f7f.node.blacksmith.redis-cluster-big-d40ed344-e4dd-4ae8-b9bb-87d461405c21.bosh" CLUSTER NODES | grep myself | awk '{print $1}')
redis 6573d26c-225f-4a12-ad63-dfdc09f1ceaa.node.blacksmith.redis-cluster-big-d40ed344-e4dd-4ae8-b9bb-87d461405c21.bosh CLUSTER REPLICATE "$MASTER"
log "configuration set"
```

Notice that `6573d26c-225f-4a12-ad63-dfdc09f1ceaa.node.blacksmith.redis-cluster-big-d40ed344-e4dd-4ae8-b9bb-87d461405c21.bosh` is the only instance set as slave.

#### Resolution: Script Modification for Correct Slave Assignment

The script responsible for initializing the cluster and assigning master-slave roles needed modification to ensure each master had its corresponding slave. The revised script section dynamically assigned slaves to each master:

```rb
<%
  ips = link('redis').instances.map { |instance| instance.address }
  bash_ips = ips.join(' ')
  masters = (0 .. (p('cluster.masters') - 1)).map { |i| ips[i] }
  slaves = (p('cluster.masters') .. (ips.size - 1)).map { |i| ips[i] }

  SLOTS = 16384
  slots_per_master = (SLOTS / masters.size).to_i

  cluster = {}
  masters.each_with_index do |master, index|
    first_slot = index * slots_per_master
    last_slot = ((index + 1) * slots_per_master) - 1
    last_slot = SLOTS - 1 if index == masters.size - 1 # Adjust for the last master

    assigned_slaves = slaves.each_slice((slaves.size / masters.size).to_i).to_a
    cluster[master] = {
      'slots' => (first_slot..last_slot).to_a,
      'slaves' => assigned_slaves[index] # Assign slaves to each master
    }
  end
%>
```

This approach ensures that each master in the cluster is correctly paired with its designated slave, aligning with the intended cluster architecture.

## Outcome Summary
After implementing the necessary fixes, the Redis cluster formation for both scenarios now aligns with expectations:

### Scenario 1
***Cluster, single master, single replica configuration (2 nodes)***

```
node/f3741875-df17-457f-805e-0d5a22744c0b:~# /var/vcap/packages/redis-6/bin/redis-cli --tls --cacert /var/vcap/jobs/cluster-6/config/tls/redis.ca --cert /var/vcap/jobs/cluster-6/config/tls/redis.cert --key /var/vcap/jobs/cluster-6/config/tls/redis.key -h 127.0.0.1 -p 6379 -a O7CUnSQtKZRLZLkkFpUqvQaQhQnoHyfMpf9Q2aSHqE3Z3BNSVClry5NuzDgGevh5
Warning: Using a password with '-a' or '-u' option on the command line interface may not be safe.
127.0.0.1:6379> cluster info
cluster_state:ok
cluster_slots_assigned:16384
cluster_slots_ok:16384
cluster_slots_pfail:0
cluster_slots_fail:0
cluster_known_nodes:2
cluster_size:1
cluster_current_epoch:1
cluster_my_epoch:1
cluster_stats_messages_ping_sent:4349
cluster_stats_messages_pong_sent:4360
cluster_stats_messages_meet_sent:1
cluster_stats_messages_sent:8710
cluster_stats_messages_ping_received:4360
cluster_stats_messages_pong_received:4350
cluster_stats_messages_received:8710
127.0.0.1:6379> cluster nodes
58eb02277e308f33477e6c4d483406998fa90c5c 10.168.17.3:6379@16379 myself,master - 0 0 1 connected 0-16383
876afcf72d50d8acfddc763d7b0bbb3c32d3b2fd 10.168.17.4:6379@16379 slave 58eb02277e308f33477e6c4d483406998fa90c5c 0 1704797970949 1 connected
```
* **CLI Output**: The cluster info and cluster nodes commands display the expected state, with one master and one slave, and all 16384 slots correctly assigned.

### Scenario 2
***Cluster, two masters, single replica configuration (4 nodes)***


```
node/7da94a2f-4636-430f-8ae4-4049758664b4:~# /var/vcap/packages/redis-6/bin/redis-cli --tls --cacert /var/vcap/jobs/cluster-6/config/tls/redis.ca --cert /var/vcap/jobs/cluster-6/config/tls/redis.cert --key /var/vcap/jobs/cluster-6/config/tls/redis.key -h 127.0.0.1 -p 6379 -a mlJGVVnbcX5KuJzxh4CckyOBVLMJ7TT537ZpQawkE17rrYZGv9yBJfc7qS1UBcfv
Warning: Using a password with '-a' or '-u' option on the command line interface may not be safe.
127.0.0.1:6379> cluster info
cluster_state:ok
cluster_slots_assigned:16384
cluster_slots_ok:16384
cluster_slots_pfail:0
cluster_slots_fail:0
cluster_known_nodes:4
cluster_size:2
cluster_current_epoch:2
cluster_my_epoch:1
cluster_stats_messages_ping_sent:4218
cluster_stats_messages_pong_sent:4848
cluster_stats_messages_meet_sent:3
cluster_stats_messages_sent:9069
cluster_stats_messages_ping_received:4848
cluster_stats_messages_pong_received:4221
cluster_stats_messages_received:9069
127.0.0.1:6379> cluster nodes
ae16bca025bfa992a5e0c9ec2fbdda10fac6af44 10.168.17.8:6379@16379 slave 5d75611c1b16e9424a79b40c5b624c87e6152166 0 1704797759111 2 connected
5bbd6f01c134a843db25993fd81cb71e1fc5ca09 10.168.17.7:6379@16379 slave 5be3d949f56e77d5848af97f1d8825e1c29f3270 0 1704797761121 1 connected
5d75611c1b16e9424a79b40c5b624c87e6152166 10.168.17.6:6379@16379 master - 0 1704797760116 2 connected 8192-16383
5be3d949f56e77d5848af97f1d8825e1c29f3270 10.168.17.5:6379@16379 myself,master - 0 1704797759000 1 connected 0-8191
```
* **CLI Output**: The cluster is correctly configured with two masters, each having its own replica. The cluster info and cluster nodes commands confirm the proper assignment of slots and node roles.

```
node/7da94a2f-4636-430f-8ae4-4049758664b4:~# cat /var/vcap/sys/log/cluster-6/cluster-6.log 
9854:C 09 Jan 2024 09:44:32.667 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
9854:C 09 Jan 2024 09:44:32.667 # Redis version=6.2.12, bits=64, commit=00000000, modified=0, pid=9854, just started
9854:C 09 Jan 2024 09:44:32.667 # Configuration loaded
9854:M 09 Jan 2024 09:44:32.675 * Increased maximum number of open files to 10032 (it was originally set to 1024).
9854:M 09 Jan 2024 09:44:32.675 * monotonic clock: POSIX clock_gettime
9854:M 09 Jan 2024 09:44:32.680 # Warning: Could not create server TCP listening socket ::*:6379: unable to bind socket, errno: 97
9854:M 09 Jan 2024 09:44:32.680 * No cluster configuration found, I'm 5be3d949f56e77d5848af97f1d8825e1c29f3270
9854:M 09 Jan 2024 09:44:32.692 # Warning: Could not create server TCP listening socket ::*:16379: unable to bind socket, errno: 97
9854:M 09 Jan 2024 09:44:32.693 * Running mode=cluster, port=6379.
9854:M 09 Jan 2024 09:44:32.693 # Server initialized
9854:M 09 Jan 2024 09:44:32.694 * Ready to accept connections
9854:M 09 Jan 2024 09:45:16.972 # IP address for this node updated to 10.168.17.5
9854:M 09 Jan 2024 09:45:17.767 * Replica 10.168.17.7:6379 asks for synchronization
9854:M 09 Jan 2024 09:45:17.767 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for '617e02a7db9300b6e363da1626f8325f2e99b5b7', my replication IDs are '2b3f5b12e9712cf8d89fa56b2223c104496f3d0e' and '0000000000000000000000000000000000000000')
9854:M 09 Jan 2024 09:45:17.767 * Replication backlog created, my new replication IDs are '3b0c9f8d7525c186482e1e9d969e7e5192d36dc9' and '0000000000000000000000000000000000000000'
9854:M 09 Jan 2024 09:45:17.768 * Starting BGSAVE for SYNC with target: disk
9854:M 09 Jan 2024 09:45:17.768 * Background saving started by pid 10217
10217:C 09 Jan 2024 09:45:17.775 * DB saved on disk
10217:C 09 Jan 2024 09:45:17.776 * RDB: 0 MB of memory used by copy-on-write
9854:M 09 Jan 2024 09:45:17.858 * Background saving terminated with success
9854:M 09 Jan 2024 09:45:17.858 * Synchronization with replica 10.168.17.7:6379 succeeded
9854:M 09 Jan 2024 09:45:22.585 # Cluster state changed: ok
```
```log
node/7da94a2f-4636-430f-8ae4-4049758664b4:~# cat /var/vcap/sys/log/cluster-6/post-deploy.stderr.log
...
[Tue Jan  9 09:45:16 UTC 2024] introducing nodes to each other
[Tue Jan  9 09:45:16 UTC 2024] Introducing 7da94a2f-4636-430f-8ae4-4049758664b4.node.blacksmith.redis-cluster-big-e82cdc10-8284-41a0-90d2-6b41606452c8.bosh (10.168.17.5) to 73816dbb-3916-4c32-9ce7-7a75480f821a.node.blacksmith.redis-cluster-big-e82cdc10-8284-41a0-90d2-6b41606452c8.bosh (10.168.17.6)
OK
OK
[Tue Jan  9 09:45:16 UTC 2024] Introducing 7da94a2f-4636-430f-8ae4-4049758664b4.node.blacksmith.redis-cluster-big-e82cdc10-8284-41a0-90d2-6b41606452c8.bosh (10.168.17.5) to a2930778-adc4-4cf7-8354-77770a799ed2.node.blacksmith.redis-cluster-big-e82cdc10-8284-41a0-90d2-6b41606452c8.bosh (10.168.17.7)
OK
OK
[Tue Jan  9 09:45:17 UTC 2024] Introducing 7da94a2f-4636-430f-8ae4-4049758664b4.node.blacksmith.redis-cluster-big-e82cdc10-8284-41a0-90d2-6b41606452c8.bosh (10.168.17.5) to cbfaa4e3-86c7-478e-b16f-b2c67a77cf21.node.blacksmith.redis-cluster-big-e82cdc10-8284-41a0-90d2-6b41606452c8.bosh (10.168.17.8)
OK
OK
[Tue Jan  9 09:45:17 UTC 2024] Introducing 73816dbb-3916-4c32-9ce7-7a75480f821a.node.blacksmith.redis-cluster-big-e82cdc10-8284-41a0-90d2-6b41606452c8.bosh (10.168.17.6) to a2930778-adc4-4cf7-8354-77770a799ed2.node.blacksmith.redis-cluster-big-e82cdc10-8284-41a0-90d2-6b41606452c8.bosh (10.168.17.7)
OK
OK
[Tue Jan  9 09:45:17 UTC 2024] Introducing 73816dbb-3916-4c32-9ce7-7a75480f821a.node.blacksmith.redis-cluster-big-e82cdc10-8284-41a0-90d2-6b41606452c8.bosh (10.168.17.6) to cbfaa4e3-86c7-478e-b16f-b2c67a77cf21.node.blacksmith.redis-cluster-big-e82cdc10-8284-41a0-90d2-6b41606452c8.bosh (10.168.17.8)
OK
OK
[Tue Jan  9 09:45:17 UTC 2024] Introducing a2930778-adc4-4cf7-8354-77770a799ed2.node.blacksmith.redis-cluster-big-e82cdc10-8284-41a0-90d2-6b41606452c8.bosh (10.168.17.7) to cbfaa4e3-86c7-478e-b16f-b2c67a77cf21.node.blacksmith.redis-cluster-big-e82cdc10-8284-41a0-90d2-6b41606452c8.bosh (10.168.17.8)
OK
OK
[Tue Jan  9 09:45:17 UTC 2024] Node introductions complete
```

* **Logs Analysis**:
    *  `/var/vcap/sys/log/cluster-6/cluster-6.log`
    the standard Redis log return the initialization sequence including the node introduction and synchronization.
    * `/var/vcap/sys/log/cluster-6/post-deploy.stderr.log`
    the post deploy log shows the correct number of node introductions. Note that each node pair appear once.


```
node/7da94a2f-4636-430f-8ae4-4049758664b4:~# cat /var/vcap/jobs/cluster-6/bin/post-deploy 
#!/bin/bash
...
# Set up slaves

  MASTER_ID=$(redis "7da94a2f-4636-430f-8ae4-4049758664b4.node.blacksmith.redis-cluster-big-e82cdc10-8284-41a0-90d2-6b41606452c8.bosh" CLUSTER NODES | grep myself | awk '{print $1}')
  
    redis "a2930778-adc4-4cf7-8354-77770a799ed2.node.blacksmith.redis-cluster-big-e82cdc10-8284-41a0-90d2-6b41606452c8.bosh" CLUSTER REPLICATE "$MASTER_ID"
  

  MASTER_ID=$(redis "73816dbb-3916-4c32-9ce7-7a75480f821a.node.blacksmith.redis-cluster-big-e82cdc10-8284-41a0-90d2-6b41606452c8.bosh" CLUSTER NODES | grep myself | awk '{print $1}')
  
    redis "cbfaa4e3-86c7-478e-b16f-b2c67a77cf21.node.blacksmith.redis-cluster-big-e82cdc10-8284-41a0-90d2-6b41606452c8.bosh" CLUSTER REPLICATE "$MASTER_ID"
```

* **Post Deploy Script**: we can see the slave setup from the resulting post-deploy script to return the expected pairs

### Key Takeaways
* **DNS vs IP Issue Resolved**: The `CLUSTER MEET` command now correctly uses IPs resolved from DNS entries, ensuring proper node introductions.
* **TLS Replication**: The inclusion of `tls-replication yes` in the configuration files ensured successful secure communication between nodes.
* **SYNC Command Reinstated**: Re-enabling the `SYNC` command was crucial for proper replication and synchronization of nodes.
* **Master Authentication**: Implementing `masterauth` in the configuration files resolved authentication issues during replication.
* **Accurate Slave Assignment**: The modified script now correctly assigns slaves to their respective masters in multiple master-slave configurations.

## Conclusion
As we reach the end of our detailed exploration into Redis cluster troubleshooting, it's clear that the journey has been as much about understanding the nuances of Redis as about mastering the art of troubleshooting itself. While the specific fixes applied were tailored to the unique challenges of the Redis environment, several key takeaways resonate more broadly:

* **Precision in Configuration**: The importance of precise configuration cannot be overstated in a Redis cluster setup. A single misconfiguration can cascade into significant issues, as seen in TLS replication challenges.

* **Detailed Log Analysis**: The role of log analysis in troubleshooting is paramount. By diving deep into the logs, we were able to decipher complex error messages and trace back to the root causes, exemplified by the SYNC missing and masterauth issues.

* **Iterative Troubleshooting**: The process reinforced an iterative approach to problem-solving. Each fix revealed another layer of the issue, demanding a step-by-step resolution strategy.

* **Impact of Infrastructure Code Changes**: The scenarios underscored how infrastructure changes can have far-reaching impacts. A change in one area can necessitate adjustments elsewhere, as observed in `CLUSTER MEET`` commands


In essence, this guide has not only been about navigating the complexities of a Redis cluster but also about adopting a methodical and comprehensive approach to problem-solving in complex systems. The lessons learned here, while rooted in Redis, provide a valuable framework for similar technical challenges, reinforcing the need for thoroughness, patience, and continuous learning.



